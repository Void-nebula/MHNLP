{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers\n",
    "from huggingface_hub import login\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_compute_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_quant_type=\"nf8\",\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    ") if device!='cpu' else None\n",
    "\n",
    "pipeline_kwargs = {\n",
    "    # 'max_length'=4096,\n",
    "    # 'max_new_tokens' : 50,\n",
    "    # 'min_length': 100,\n",
    "    'early_stopping':True,\n",
    "    'do_sample':False,\n",
    "    'top_k':1,\n",
    "    # 'top_p':0.9,\n",
    "    'temperature':1.0,\n",
    "    # 'num_return_sequences':1,\n",
    "    # 'no_repeat_ngram_size': 2,\n",
    "    'return_full_text':False,\n",
    "    # 'eos_token_id':tokenizer.eos_token_id\n",
    "    # 'max_length':2000,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8b845fba72475fa99690ca56bc0640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure the pipeline for Causal Language Modeling\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_prompt_classification_llama_desc(row):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    <s> [INST] <<SYS>>\n",
    "    You are a professional therapist. Your patient is telling you about their basic information and some answers to a mental health servey. Based on the information, please infer whether the patient has depression and/or anxiety disorders.\n",
    "    \n",
    "    Please answer with only one digit: 0, 1, where 0 means the patient does not have depression or anxiety disorders, and 1 means the patient has depression and/or anxiety disorders. For example, if the input indicates the patient don't have depression or anxiety disorder, your answer shuold be: 0. Otherwise, your answer should be: 1. You don't need to explain your answer.</s>\n",
    "    <</SYS>>\n",
    "\n",
    "    [/INST]\n",
    "\n",
    "    Sure, let me know the responses that the patient said. I'll try to classify if they have depression and/or anxiety disorders. </s>\n",
    "\n",
    "    </s>\n",
    "\n",
    "    <s> [INST]\n",
    "\n",
    "    The responses are as follows:\n",
    "\n",
    "    {row[\"prompt\"]}\n",
    "\n",
    "    [/INST]\n",
    "\n",
    "    I think the digit should be: </s>\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/txie/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/txie/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/home/txie/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: \"1\\n\\n    Based on the patient's responses, it seems that they have depression and anxiety disorders. The patient reported that they have always been in a stressful financial situation, which can be a",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     msg \u001b[38;5;241m=\u001b[39m prep_prompt_classification_llama_desc(row)\n\u001b[1;32m      5\u001b[0m     response \u001b[38;5;241m=\u001b[39m pipeline(msg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpipeline_kwargs)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# df['predict'] = results\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# df.to_csv('output.csv', index=False)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(results)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: \"1\\n\\n    Based on the patient's responses, it seems that they have depression and anxiety disorders. The patient reported that they have always been in a stressful financial situation, which can be a"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"HMS_2023_processed_binary_balanced_1000_test.csv\")\n",
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    msg = prep_prompt_classification_llama_desc(row)\n",
    "    response = pipeline(msg, **pipeline_kwargs)[0]['generated_text']\n",
    "    results.append(int(response))\n",
    "    # df['predict'] = results\n",
    "    # df.to_csv('output.csv', index=False)\n",
    "\n",
    "y_pred = np.array(results)\n",
    "y_true = np.array(df['label'])\n",
    "\n",
    "accuracy = np.mean(y_pred == y_true)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
