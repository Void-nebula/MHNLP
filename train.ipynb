{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaModel, RobertaPreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRobertaModel(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # Roberta base model without pooling layer\n",
    "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        # Classifier for overall symptom presence: 4 labels (both, depression, anxiety, none)\n",
    "        self.symptom_classifier = nn.Linear(config.hidden_size, 4)\n",
    "        \n",
    "        # Classifier for depression states: 6 labels (0-5)\n",
    "        self.depression_classifier = nn.Linear(config.hidden_size, 6)\n",
    "        \n",
    "        # Classifier for anxiety states: 7 labels (0-6)\n",
    "        self.anxiety_classifier = nn.Linear(config.hidden_size, 7)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # Uncertainty Weight Parameters\n",
    "        self.sigma_symptom = nn.Parameter(torch.ones(1))\n",
    "        self.sigma_depression = nn.Parameter(torch.ones(1))\n",
    "        self.sigma_anxiety = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, depression_labels=None, anxiety_labels=None):\n",
    "        # Get Roberta outputs\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get pooled [CLS] token output\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        # Overall symptom classification logits\n",
    "        symptom_logits = self.symptom_classifier(pooled_output)\n",
    "        \n",
    "        # Depression state classification logits\n",
    "        depression_logits = self.depression_classifier(pooled_output)\n",
    "\n",
    "        # Anxiety state classification logits\n",
    "        anxiety_logits = self.anxiety_classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None and depression_labels is not None and anxiety_labels is not None:\n",
    "            # Loss function\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Calculate individual losses for each task\n",
    "            symptom_loss = loss_fct(symptom_logits, labels)\n",
    "            depression_loss = loss_fct(depression_logits, depression_labels)\n",
    "            anxiety_loss = loss_fct(anxiety_logits, anxiety_labels)\n",
    "\n",
    "            # Combine losses\n",
    "            # loss = symptom_loss + depression_loss + anxiety_loss\n",
    "            \n",
    "            # Uncertainty Weighted Loss\n",
    "            loss = (\n",
    "            (1 / (2 * self.sigma_symptom ** 2)) * symptom_loss +\n",
    "            (1 / (2 * self.sigma_depression ** 2)) * depression_loss +\n",
    "            (1 / (2 * self.sigma_anxiety ** 2)) * anxiety_loss +\n",
    "            torch.log(self.sigma_symptom * self.sigma_depression * self.sigma_anxiety)\n",
    "            )\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=(symptom_logits, depression_logits, anxiety_logits),\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_symptom_correct = 0\n",
    "    total_depression_correct = 0\n",
    "    total_anxiety_correct = 0\n",
    "\n",
    "    total_items = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)  # Symptom classification labels\n",
    "            depression_labels = batch['depression_labels'].to(device)  # Depression state labels\n",
    "            anxiety_labels = batch['anxiety_labels'].to(device)  # Anxiety state labels\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Unpack logits\n",
    "            symptom_logits = outputs.logits[0]  # Symptom classification logits\n",
    "            depression_logits = outputs.logits[1]  # Depression state logits\n",
    "            anxiety_logits = outputs.logits[2]  # Anxiety state logits\n",
    "\n",
    "            # Symptom classification predictions\n",
    "            _, symptom_preds = torch.max(symptom_logits, dim=1)\n",
    "            total_symptom_correct += torch.sum(symptom_preds == labels)\n",
    "\n",
    "            # Depression state classification predictions (only if predicted as depression or both)\n",
    "            _, depression_preds = torch.max(depression_logits, dim=1)\n",
    "            total_depression_correct += torch.sum(depression_preds == depression_labels)\n",
    "\n",
    "            # Anxiety state classification predictions (only if predicted as anxiety or both)\n",
    "            _, anxiety_preds = torch.max(anxiety_logits, dim=1)\n",
    "            total_anxiety_correct += torch.sum(anxiety_preds == anxiety_labels)\n",
    "\n",
    "            # Count total items\n",
    "            total_items += labels.size(0)\n",
    "\n",
    "    # Compute accuracies\n",
    "    symptom_accuracy = total_symptom_correct.double() / total_items\n",
    "    depression_accuracy = total_depression_correct.double() / total_items\n",
    "    anxiety_accuracy = total_anxiety_correct.double() / total_items\n",
    "\n",
    "    print(f'Symptom Classification Accuracy: {symptom_accuracy:.4f}')\n",
    "    print(f'Depression State Accuracy: {depression_accuracy:.4f}')\n",
    "    print(f'Anxiety State Accuracy: {anxiety_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Define the dataset class to handle the CSV\n",
    "    class MentalHealthDataset(Dataset):\n",
    "        def __init__(self, file_path, tokenizer, max_length=512):\n",
    "            self.data = pd.read_csv(file_path)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            # Extract the text and the labels from the dataset\n",
    "            text = self.data.loc[index, 'text']\n",
    "            symptom_label = int(self.data.loc[index, 'disorder'])  # overall disorder label (both, depression, anxiety, none)\n",
    "            depression_label = int(self.data.loc[index, 'depression_state'])  # depression state (0-5)\n",
    "            anxiety_label = int(self.data.loc[index, 'anxiety_state'])  # anxiety state (0-6)\n",
    "\n",
    "            # Tokenize the text\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # Return a dictionary of inputs and labels\n",
    "            return {\n",
    "                'input_ids': inputs['input_ids'].squeeze(),  # remove the batch dimension\n",
    "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                'labels': torch.tensor(symptom_label, dtype=torch.long),\n",
    "                'depression_labels': torch.tensor(depression_label, dtype=torch.long),\n",
    "                'anxiety_labels': torch.tensor(anxiety_label, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = MentalHealthDataset('Datasets/584_project/train_data_customize_hybrid_class_classification_depression.csv', tokenizer)\n",
    "    val_dataset = MentalHealthDataset('Datasets/584_project/val_data_customize_hybrid_class_classification_depression.csv', tokenizer)\n",
    "    test_dataset = MentalHealthDataset('Datasets/584_project/test_data_customize_hybrid_class_classification_depression.csv', tokenizer)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create DataLoader for each dataset\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Load configuration for Roberta\n",
    "    config = RobertaConfig.from_pretrained('roberta-base')\n",
    "\n",
    "    # Initialize your multi-task classification model\n",
    "    model = MultiTaskRobertaModel(config)\n",
    "\n",
    "    # Define optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            depression_labels = batch['depression_labels'].to(device)\n",
    "            anxiety_labels = batch['anxiety_labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                            labels=labels, depression_labels=depression_labels, anxiety_labels=anxiety_labels)\n",
    "\n",
    "            # compute loss\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % 2 == 0:\n",
    "                print(f\"Step {step}/{len(train_loader)} - Current Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # torch.save(model, \"multi_task_roberta_full_model.pth\")\n",
    "    # Evaluate on validation set\n",
    "    evaluate_model(model, val_loader)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/207 - Current Loss: 2.6687\n",
      "Step 2/207 - Current Loss: 2.4046\n",
      "Step 4/207 - Current Loss: 2.2502\n",
      "Step 6/207 - Current Loss: 2.4724\n",
      "Step 8/207 - Current Loss: 2.3491\n",
      "Step 10/207 - Current Loss: 2.4100\n",
      "Step 12/207 - Current Loss: 2.3811\n",
      "Step 14/207 - Current Loss: 2.3945\n",
      "Step 16/207 - Current Loss: 2.5756\n",
      "Step 18/207 - Current Loss: 2.4554\n",
      "Step 20/207 - Current Loss: 2.3375\n",
      "Step 22/207 - Current Loss: 2.4228\n",
      "Step 24/207 - Current Loss: 2.3871\n",
      "Step 26/207 - Current Loss: 2.3479\n",
      "Step 28/207 - Current Loss: 2.3491\n",
      "Step 30/207 - Current Loss: 2.4090\n",
      "Step 32/207 - Current Loss: 2.3182\n",
      "Step 34/207 - Current Loss: 2.2862\n",
      "Step 36/207 - Current Loss: 2.1851\n",
      "Step 38/207 - Current Loss: 2.2052\n",
      "Step 40/207 - Current Loss: 2.2364\n",
      "Step 42/207 - Current Loss: 2.4126\n",
      "Step 44/207 - Current Loss: 2.2922\n",
      "Step 46/207 - Current Loss: 2.5126\n",
      "Step 48/207 - Current Loss: 2.3155\n",
      "Step 50/207 - Current Loss: 2.3391\n",
      "Step 52/207 - Current Loss: 2.3620\n",
      "Step 54/207 - Current Loss: 2.5046\n",
      "Step 56/207 - Current Loss: 2.4947\n",
      "Step 58/207 - Current Loss: 2.5073\n",
      "Step 60/207 - Current Loss: 2.4313\n",
      "Step 62/207 - Current Loss: 2.3660\n",
      "Step 64/207 - Current Loss: 2.3898\n",
      "Step 66/207 - Current Loss: 2.3169\n",
      "Step 68/207 - Current Loss: 2.3399\n",
      "Step 70/207 - Current Loss: 2.3048\n",
      "Step 72/207 - Current Loss: 2.5027\n",
      "Step 74/207 - Current Loss: 2.4508\n",
      "Step 76/207 - Current Loss: 2.4826\n",
      "Step 78/207 - Current Loss: 2.3931\n",
      "Step 80/207 - Current Loss: 2.1542\n",
      "Step 82/207 - Current Loss: 2.3727\n",
      "Step 84/207 - Current Loss: 2.3998\n",
      "Step 86/207 - Current Loss: 2.3652\n",
      "Step 88/207 - Current Loss: 2.3547\n",
      "Step 90/207 - Current Loss: 2.4144\n",
      "Step 92/207 - Current Loss: 2.4457\n",
      "Step 94/207 - Current Loss: 2.4155\n",
      "Step 96/207 - Current Loss: 2.3945\n",
      "Step 98/207 - Current Loss: 2.3274\n",
      "Step 100/207 - Current Loss: 2.4509\n",
      "Step 102/207 - Current Loss: 2.2796\n",
      "Step 104/207 - Current Loss: 2.2609\n",
      "Step 106/207 - Current Loss: 2.3706\n",
      "Step 108/207 - Current Loss: 2.2848\n",
      "Step 110/207 - Current Loss: 2.3634\n",
      "Step 112/207 - Current Loss: 2.5161\n",
      "Step 114/207 - Current Loss: 2.4016\n",
      "Step 116/207 - Current Loss: 2.3418\n",
      "Step 118/207 - Current Loss: 2.3235\n",
      "Step 120/207 - Current Loss: 2.4091\n",
      "Step 122/207 - Current Loss: 2.3255\n",
      "Step 124/207 - Current Loss: 2.4806\n",
      "Step 126/207 - Current Loss: 2.3225\n",
      "Step 128/207 - Current Loss: 2.4648\n",
      "Step 130/207 - Current Loss: 2.3659\n",
      "Step 132/207 - Current Loss: 2.3711\n",
      "Step 134/207 - Current Loss: 2.3584\n",
      "Step 136/207 - Current Loss: 2.3501\n",
      "Step 138/207 - Current Loss: 2.3093\n",
      "Step 140/207 - Current Loss: 2.3624\n",
      "Step 142/207 - Current Loss: 2.3727\n",
      "Step 144/207 - Current Loss: 2.2289\n",
      "Step 146/207 - Current Loss: 2.3726\n",
      "Step 148/207 - Current Loss: 2.4217\n",
      "Step 150/207 - Current Loss: 2.3437\n",
      "Step 152/207 - Current Loss: 2.3778\n",
      "Step 154/207 - Current Loss: 2.2652\n",
      "Step 156/207 - Current Loss: 2.4155\n",
      "Step 158/207 - Current Loss: 2.4445\n",
      "Step 160/207 - Current Loss: 2.3840\n",
      "Step 162/207 - Current Loss: 2.3946\n",
      "Step 164/207 - Current Loss: 2.4157\n",
      "Step 166/207 - Current Loss: 2.2788\n",
      "Step 168/207 - Current Loss: 2.2878\n",
      "Step 170/207 - Current Loss: 2.3675\n",
      "Step 172/207 - Current Loss: 2.3702\n",
      "Step 174/207 - Current Loss: 2.3489\n",
      "Step 176/207 - Current Loss: 2.4670\n",
      "Step 178/207 - Current Loss: 2.4187\n",
      "Step 180/207 - Current Loss: 2.2921\n",
      "Step 182/207 - Current Loss: 2.3205\n",
      "Step 184/207 - Current Loss: 2.4030\n",
      "Step 186/207 - Current Loss: 2.3093\n",
      "Step 188/207 - Current Loss: 2.3195\n",
      "Step 190/207 - Current Loss: 2.3312\n",
      "Step 192/207 - Current Loss: 2.2691\n",
      "Step 194/207 - Current Loss: 2.3433\n",
      "Step 196/207 - Current Loss: 2.3981\n",
      "Step 198/207 - Current Loss: 2.4683\n",
      "Step 200/207 - Current Loss: 2.2571\n",
      "Step 202/207 - Current Loss: 2.3959\n",
      "Step 204/207 - Current Loss: 2.4503\n",
      "Step 206/207 - Current Loss: 2.3009\n",
      "Epoch 1, Loss: 2.3724\n",
      "Symptom Classification Accuracy: 0.1886\n",
      "Depression State Accuracy: 0.7574\n",
      "Anxiety State Accuracy: 0.2582\n",
      "Symptom Classification Accuracy: 0.1900\n",
      "Depression State Accuracy: 0.7581\n",
      "Anxiety State Accuracy: 0.2638\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
